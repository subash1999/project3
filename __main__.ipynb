{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os,time,sys\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing.Normalize import Normalize\n",
    "import helper.SeriesHelper as series_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_matrix = Normalize().get_normalized_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = normal_matrix.columns\n",
    "index = normal_matrix.index\n",
    "X = normal_matrix.to_numpy()\n",
    "y = series_helper.get_relapse_value_from_series_matrix(normal_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, SelectPercentile, SelectFdr, SelectFpr, SelectFwe\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import RFE,RFECV\n",
    "from sklearn.feature_selection import chi2,f_classif, mutual_info_classif\n",
    "from sklearn.model_selection import ParameterGrid, GridSearchCV\n",
    "from sklearn.svm import LinearSVC, NuSVC, SVC\n",
    "from sklearn.linear_model import LogisticRegression,LassoCV, Lasso\n",
    "from sklearn.ensemble import ExtraTreesClassifier, BaggingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB\n",
    "from xgboost import XGBClassifier\n",
    "from tpot import TPOTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_func = [chi2,f_classif,mutual_info_classif]\n",
    "k = [num for num in range(100,1050,100)]\n",
    "percentile = [perc for perc in range(5,15,3)]\n",
    "alpha = [alpha/1000 for alpha in range(0,55,5)]\n",
    "penalty = ['l1','l2']\n",
    "neighbors = [x for x in range(5,50,10)]\n",
    "estimator = []\n",
    "# estimator += [KNeighborsClassifier(n_neighbors = neighbor) for neighbor in neighbors]\n",
    "# estimator += [LinearSVC(penalty=x,dual=False) for x in penalty] \n",
    "# estimator += [NuSVC(),SVC()]\n",
    "# estimator += [SGDClassifier(penalty=x) for x in penalty] \n",
    "estimator += [LogisticRegression(penalty=x,dual=False) for x in penalty] \n",
    "# estimator += [GaussianNB(), MultinomialNB(), ComplementNB(), BernoulliNB()]\n",
    "# estimator += [LassoCV(cv=5,n_jobs=7)]\n",
    "estimator += [ExtraTreesClassifier(bootstrap=True,n_jobs=4)]\n",
    "estimator += [ExtraTreesClassifier(n_jobs=4)]\n",
    "estimator += [XGBClassifier()]\n",
    "# estimator += [BaggingClassifier(bootstrap=True,n_jobs=4)]\n",
    "# estimator += [BaggingClassifier(n_jobs=4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection\n",
    "select_k_best_grid = [{'score_func': score_func,'k': k}]\n",
    "select_percentile_grid = [{'score_func': score_func,'percentile': percentile}]\n",
    "select_fdr_grid = [{'score_func': score_func,'alpha': alpha}]\n",
    "select_fpr_grid = [{'score_func': score_func,'alpha': alpha}]\n",
    "select_fwe_grid = [{'score_func': score_func,'alpha': alpha}]\n",
    "# doesnot goes with the GridSearchCV, cross_val_score\n",
    "select_from_model_grid = [{'estimator' : estimator}]\n",
    "rfe_grid  = [{'estimator' : estimator,'n_features_to_select' : k, 'step' : [50]}]\n",
    "rfecv_grid = [{'estimator' : estimator, 'min_features_to_select' : [100], 'cv' : [10],'n_jobs' : [4]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "{\n",
    "    'feature_selection': [PCA(iterated_power=7)],\n",
    "    'feature_selection__n_components': [100,200,300,400,500,600],\n",
    "    'model' : estimator\n",
    "},\n",
    "{\n",
    "    'feature_selection': [NMF()],\n",
    "    'feature_selection__n_components': [100],\n",
    "    'model' : estimator\n",
    "},\n",
    "{\n",
    "    'feature_selection': [SelectKBest()],\n",
    "    'feature_selection__score_func': select_k_best_grid[0]['score_func'],\n",
    "    'feature_selection__k' : select_k_best_grid[0]['k'], \n",
    "    'model' : estimator\n",
    "},\n",
    "{\n",
    "    'feature_selection': [SelectPercentile()],\n",
    "    'feature_selection__score_func': select_percentile_grid[0]['score_func'],\n",
    "    'feature_selection__percentile' : select_percentile_grid[0]['percentile'], \n",
    "    'model' : estimator  \n",
    "},\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps = [('feature_selection',SelectKBest()),('model',estimator[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = ParameterGrid(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(pipeline,param_grid=param_grid,n_jobs = -1,cv=5,return_train_score=True,verbose=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Optimization Progress:  23%|██▎       | 68/300 [28:06<5:50:04, 90.54s/pipeline]"
    }
   ],
   "source": [
    "tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2, random_state=42,n_jobs=7)\n",
    "tpot.fit(X_train, y_train)\n",
    "print(tpot.score(X_test, y_test))\n",
    "tpot.export('tpot_.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "C:\\Users\\subas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\nC:\\Users\\subas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n                   multi_class='warn', n_jobs=None, penalty='l1',\n                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n                   warm_start=False), (837, 439), 0.996415770609319, 0.6166666666666667]\nC:\\Users\\subas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\nC:\\Users\\subas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n                   multi_class='warn', n_jobs=None, penalty='l2',\n                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n                   warm_start=False), (837, 342), 1.0, 0.6166666666666667]\n[ExtraTreesClassifier(bootstrap=True, class_weight=None, criterion='gini',\n                     max_depth=None, max_features='auto', max_leaf_nodes=None,\n                     min_impurity_decrease=0.0, min_impurity_split=None,\n                     min_samples_leaf=1, min_samples_split=2,\n                     min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=4,\n                     oob_score=False, random_state=None, verbose=0,\n                     warm_start=False), (837, 351), 0.973715651135006, 0.6083333333333333]\n[ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n                     max_depth=None, max_features='auto', max_leaf_nodes=None,\n                     min_impurity_decrease=0.0, min_impurity_split=None,\n                     min_samples_leaf=1, min_samples_split=2,\n                     min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=4,\n                     oob_score=False, random_state=None, verbose=0,\n                     warm_start=False), (837, 356), 1.0, 0.6194444444444445]\n[XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, gamma=0,\n              learning_rate=0.1, max_delta_step=0, max_depth=3,\n              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n              nthread=None, objective='binary:logistic', random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n              silent=None, subsample=1, verbosity=1), (837, 288), 1.0, 0.6416666666666667]\n"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['Model','dimension','train_score','test_score'])\n",
    "for model in estimator:\n",
    "    data = [model]\n",
    "    pca = PCA().fit(X_train,y_train)\n",
    "    select = SelectFromModel(model).fit(pca.transform(X_train),y_train)\n",
    "    temp_train = select.transform(pca.transform(X_train))\n",
    "    data.append(temp_train.shape)\n",
    "    temp_test = select.transform(pca.transform(X_test))\n",
    "    model.fit(temp_train,y_train)\n",
    "    data.append(model.score(temp_train,y_train))\n",
    "    data.append(model.score(temp_test,y_test))\n",
    "    df = df.append(pd.Series(data),ignore_index=True)\n",
    "    print(data)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pickle(oobject,filename):\n",
    "        with open(filename+'.pickle', 'wb') as f:\n",
    "            pickle.dump(oobject, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}