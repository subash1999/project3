{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36864bit5cbe49f610aa411e8c53eb13e742ec54",
   "display_name": "Python 3.6.8 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, SelectPercentile, SelectFdr, SelectFpr, SelectFwe\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import RFE,RFECV\n",
    "from sklearn.feature_selection import chi2,f_classif, mutual_info_classif\n",
    "from sklearn.model_selection import ParameterGrid, GridSearchCV\n",
    "from sklearn.svm import LinearSVC, NuSVC, SVC\n",
    "from sklearn.linear_model import LogisticRegression,LassoCV, Lasso\n",
    "from sklearn.ensemble import ExtraTreesClassifier, BaggingClassifier, VotingClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB\n",
    "from xgboost import XGBClassifier\n",
    "from tpot import TPOTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing.Normalize import Normalize\n",
    "import helper.SeriesHelper as series_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_matrix = Normalize().get_normalized_data()\n",
    "X = normal_matrix.to_numpy()\n",
    "y = series_helper.get_relapse_value_from_series_matrix(normal_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(normal_matrix.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jobs = 6\n",
    "verbose =  1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = Pipeline(memory=None,\n",
    "         steps=[('features',\n",
    "                 FeatureUnion(n_jobs=n_jobs,\n",
    "                              transformer_list=[('pca',\n",
    "                                                 PCA(copy=True,\n",
    "                                                     iterated_power='auto',\n",
    "                                                     n_components=100,\n",
    "                                                     random_state=None,\n",
    "                                                     svd_solver='auto', tol=0.0,\n",
    "                                                     whiten=False)),\n",
    "                                                ('univ_select',\n",
    "                                                 SelectKBest(k=500,\n",
    "                                                             ))],\n",
    "                              transformer_weights=None, verbose=verbose)),\n",
    "                ('svm',\n",
    "                 SVC(C=0.1, break_ties=False, cache_size=200, class_weight=None,\n",
    "                     coef0=0.0, decision_function_shape='ovr', degree=3,\n",
    "                     gamma='scale', kernel='linear', max_iter=-1,\n",
    "                     probability=False, random_state=None, shrinking=True,\n",
    "                     tol=0.001, verbose=False))],\n",
    "         verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = Pipeline(memory=None,\n",
    "         steps=[('feature_selection',\n",
    "                 NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0,\n",
    "                     max_iter=200, n_components=100, random_state=None,\n",
    "                     shuffle=False, solver='cd', tol=0.0001, verbose=0)),\n",
    "                ('model',\n",
    "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
    "                                    fit_intercept=True, intercept_scaling=1,\n",
    "                                    l1_ratio=None, max_iter=100,\n",
    "                                    multi_class='auto', n_jobs=n_jobs,\n",
    "                                    penalty='l2', random_state=None,\n",
    "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
    "                                    warm_start=False))],\n",
    "         verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3 = Pipeline(memory=None,\n",
    "         steps=[('feature_selection',\n",
    "                 NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0,\n",
    "                     max_iter=200, n_components=100, random_state=None,\n",
    "                     shuffle=False, solver='cd', tol=0.0001, verbose=0)),\n",
    "                ('model',\n",
    "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
    "                                    fit_intercept=True, intercept_scaling=1,\n",
    "                                    l1_ratio=None, max_iter=100,\n",
    "                                    multi_class='auto', n_jobs=n_jobs,\n",
    "                                    penalty='l2', random_state=None,\n",
    "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
    "                                    warm_start=False))],\n",
    "         verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "m4 = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
    "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
    "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=n_jobs,\n",
    "              nthread=None, objective='binary:logistic', random_state=0,\n",
    "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "              silent=None, subsample=1, verbosity=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "m5 = Pipeline(memory=None,\n",
    "         steps=[('feature_selection',\n",
    "                 NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0,\n",
    "                     max_iter=300, n_components=100, random_state=None,\n",
    "                     shuffle=False, solver='cd', tol=0.0001, verbose=0)),\n",
    "                     ('model',\n",
    "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
    "                                    fit_intercept=True, intercept_scaling=1,\n",
    "                                    l1_ratio=None, max_iter=5000,\n",
    "                                    multi_class='auto', n_jobs=n_jobs,\n",
    "                                    penalty='l2', random_state=None,\n",
    "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
    "                                    warm_start=False))],\n",
    "         verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "m6 = Pipeline(memory=None,\n",
    "         steps=[('feature_selection',\n",
    "                 NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0,\n",
    "                     max_iter=400, n_components=100, random_state=None,\n",
    "                     shuffle=False, solver='cd', tol=0.0001, verbose=0)),\n",
    "                     ('model',\n",
    "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
    "                                    fit_intercept=True, intercept_scaling=1,\n",
    "                                    l1_ratio=None, max_iter=5000,\n",
    "                                    multi_class='auto', n_jobs=n_jobs,\n",
    "                                    penalty='l2', random_state=None,\n",
    "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
    "                                    warm_start=False))],verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'verbose'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-0ae8309df288>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m voting_hard = VotingClassifier(estimators=[\n\u001b[0;32m      2\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[1;34m'm1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mm1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'm2'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mm2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'm3'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mm3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'm4'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mm4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'm5'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mm5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'm6'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mm6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m ],voting='hard',n_jobs=n_jobs,verbose=10)\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'verbose'"
     ]
    }
   ],
   "source": [
    "voting_hard = VotingClassifier(estimators=[\n",
    "    ('m1',m1),('m2',m2),('m3',m3),('m4',m4),('m5',m5),('m6',m6)\n",
    "],voting='hard',n_jobs=n_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "--------------------------------------------------------------------------------------------------------\nFitting Started\nFitting Ended\n Train Score :  0.8252184769038702 \t Test Score  0.6590909090909091\n Precision Train :  0.9142857142857143  Precision Test :  0.5679012345679012\nTime Required :  4.534290075302124\n--------------------------------------------------------------------------------------------------------\nFitting Started\n[Pipeline] . (step 1 of 2) Processing feature_selection, total= 1.2min\n[Pipeline] ............. (step 2 of 2) Processing model, total=   0.1s\nFitting Ended\n Train Score :  0.7265917602996255 \t Test Score  0.6565656565656566\n Precision Train :  0.722972972972973  Precision Test :  0.6041666666666666\nTime Required :  113.17234492301941\n--------------------------------------------------------------------------------------------------------\nFitting Started\n[Pipeline] . (step 1 of 2) Processing feature_selection, total= 1.1min\n[Pipeline] ............. (step 2 of 2) Processing model, total=   0.1s\nFitting Ended\n Train Score :  0.7378277153558053 \t Test Score  0.648989898989899\n Precision Train :  0.7329192546583851  Precision Test :  0.5573770491803278\nTime Required :  105.38286256790161\n--------------------------------------------------------------------------------------------------------\nFitting Started\nFitting Ended\n Train Score :  1.0 \t Test Score  0.648989898989899\n Precision Train :  1.0  Precision Test :  0.5522388059701493\nTime Required :  49.04051160812378\n--------------------------------------------------------------------------------------------------------\nFitting Started\n[Pipeline] . (step 1 of 2) Processing feature_selection, total= 1.4min\n[Pipeline] ............. (step 2 of 2) Processing model, total=   0.1s\nFitting Ended\n Train Score :  0.7303370786516854 \t Test Score  0.6616161616161617\n Precision Train :  0.7254901960784313  Precision Test :  0.6071428571428571\nTime Required :  136.60047841072083\n--------------------------------------------------------------------------------------------------------\nFitting Started\n[Pipeline] . (step 1 of 2) Processing feature_selection, total= 1.9min\n[Pipeline] ............. (step 2 of 2) Processing model, total=   0.1s\nFitting Ended\n Train Score :  0.7340823970037453 \t Test Score  0.6414141414141414\n Precision Train :  0.7307692307692307  Precision Test :  0.53125\nTime Required :  185.07051348686218\n"
    }
   ],
   "source": [
    "for x in [m1,m2,m3,m4,m5,m6]:\n",
    "    t = time.time()    \n",
    "    print(\"-------------\"*8)\n",
    "    print(\"Fitting Started\")\n",
    "    x.fit(X_train,y_train)\n",
    "    print(\"Fitting Ended\")\n",
    "    print(\" Train Score : \",x.score(X_train,y_train),\"\\t Test Score \",x.score(X_test,y_test))\n",
    "    print(\" Precision Train : \",precision_score(y_train,x.predict(X_train)),\" Precision Test : \",precision_score(y_test,x.predict(X_test)))\n",
    "    print(\"Time Required : \",time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "VotingClassifier(estimators=[('m1',\n                              Pipeline(memory=None,\n                                       steps=[('features',\n                                               FeatureUnion(n_jobs=6,\n                                                            transformer_list=[('pca',\n                                                                               PCA(copy=True,\n                                                                                   iterated_power='auto',\n                                                                                   n_components=100,\n                                                                                   random_state=None,\n                                                                                   svd_solver='auto',\n                                                                                   tol=0.0,\n                                                                                   whiten=False)),\n                                                                              ('univ_select',\n                                                                               SelectKBest(k=500,\n                                                                                           score_func=<function f_classif at 0x000002B919B63378>))],\n                                                            transformer_weights=None,\n                                                            verb...\n                                                   shuffle=False, solver='cd',\n                                                   tol=0.0001, verbose=0)),\n                                              ('model',\n                                               LogisticRegression(C=1.0,\n                                                                  class_weight=None,\n                                                                  dual=False,\n                                                                  fit_intercept=True,\n                                                                  intercept_scaling=1,\n                                                                  l1_ratio=None,\n                                                                  max_iter=5000,\n                                                                  multi_class='auto',\n                                                                  n_jobs=6,\n                                                                  penalty='l2',\n                                                                  random_state=None,\n                                                                  solver='lbfgs',\n                                                                  tol=0.0001,\n                                                                  verbose=0,\n                                                                  warm_start=False))],\n                                       verbose=1))],\n                 flatten_transform=True, n_jobs=6, voting='hard', weights=None)"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_hard.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.6590909090909091"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_hard.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "2000 selected features\n"
    }
   ],
   "source": [
    "def cor_selector(X, y,num_feats):\n",
    "    cor_list = []\n",
    "    feature_name = cols\n",
    "    # calculate the correlation with y for each feature\n",
    "    for i in cols:\n",
    "        cor = np.corrcoef(X[i], y)[0, 1]\n",
    "        cor_list.append(cor)\n",
    "    # replace NaN with 0\n",
    "    cor_list = [0 if np.isnan(i) else i for i in cor_list]\n",
    "    # feature name\n",
    "    cor_feature = X.iloc[:,np.argsort(np.abs(cor_list))[-num_feats:]].columns.tolist()\n",
    "    # feature selection? 0 for not select, 1 for select\n",
    "    cor_support = [True if i in cor_feature else False for i in feature_name]\n",
    "    return cor_support, cor_feature\n",
    "cor_support, cor_feature = cor_selector(normal_matrix, y,2000)\n",
    "print(str(len(cor_feature)), 'selected features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(normal_matrix[cor_feature],y,test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=400).fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_train = nmf.transform(X_train)\n",
    "temp_test = nmf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=10000,\n                   multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n                   warm_start=False)"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(temp_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1.0"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(temp_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.803030303030303"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(temp_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# baseline model\n",
    "def create_baseline():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(11800, input_dim=11800, activation='relu'))\n",
    "    model.add(Dense(5600, activation='relu'))\n",
    "    # model.add(Dense(3000, activation='relu'))\n",
    "    # model.add(Dense(1500, activation='relu'))\n",
    "    # model.add(Dense(750, activation='relu'))\n",
    "    model.add(Dense(300, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy','binary_accuracy',])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model with standardized dataset\n",
    "estimator = KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=5, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "results = cross_val_score(estimator, X,y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}